{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4N-QZed-lsS"
   },
   "source": [
    "# CPSC 532R/533R Visual AI - Assignment 2\n",
    "\n",
    "This Jupyter notebook provides downloads and defines a pytorch dataset of egocentric images and corresponding 2D pose, a pre-defined neural network, and plotting utility functions. We also provide training code for regressing 2D pose directly from the image. All modules should seamlessly integrate into your Assignment 1 solution as they use dictionaries for storing the input images and output labels. You need to extend this or your Assignment 1 notebook with the tasks described in the Assignment2.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1578892542620,
     "user": {
      "displayName": "Helge Rhodin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDx7RzVpx1g9OElwvn2vngvWxd9Sy5QCa98TE4zPg=s64",
      "userId": "13505891862888343836"
     },
     "user_tz": 480
    },
    "id": "M3kNUT74VzSM",
    "outputId": "31ad1846-30d1-4b22-97a0-f279a2cfb370"
   },
   "outputs": [],
   "source": [
    "# download dataset from the web (400 MB file from https://www.cs.ubc.ca/~rhodin/20_CPSC_532R_533R/assignments/EgoCap_nth10.hdf5)\n",
    "file_name = \"EgoCap_nth10.hdf5\"\n",
    "import os.path\n",
    "import urllib\n",
    "if not os.path.exists(file_name):\n",
    "    print(\"Downloading dataset, might take a while... its 400 MB\")\n",
    "    urllib.request.urlretrieve(\"https://www.cs.ubc.ca/~rhodin/20_CPSC_532R_533R/assignments/\"+file_name,file_name)\n",
    "    print(\"Done downloading\")\n",
    "else:\n",
    "    print(\"Dataset already present, nothing to be done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rEw3FX9UWUxr"
   },
   "outputs": [],
   "source": [
    "# utility dictionary that can move tensor values between devices via the 'to(device)' function\n",
    "from collections import OrderedDict \n",
    "class DeviceDict(dict):\n",
    "    # following https://stackoverflow.com/questions/3387691/how-to-perfectly-override-a-dict\n",
    "    def __init__(self, *args):\n",
    "      super(DeviceDict, self).__init__(*args)\n",
    "    def to(self, device):\n",
    "      dd = DeviceDict() # return shallow copy\n",
    "      for k,v in self.items():\n",
    "          if torch.is_tensor(v):\n",
    "              dd[k] = v.to(device)\n",
    "          else:\n",
    "              dd[k] = v\n",
    "      return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uw6VMbux55VA"
   },
   "outputs": [],
   "source": [
    "# Definition the EgoCap dataset (small version)\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "class EgoCapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_folder):\n",
    "        super(EgoCapDataset).__init__();\n",
    "        data_file = 'EgoCap_nth10.hdf5'\n",
    "        print(\"Loading dataset to memory, can take some seconds\")\n",
    "        with h5py.File(data_file, 'r') as hf:\n",
    "            self.poses_2d = torch.from_numpy(hf['pose_2d'][...])\n",
    "            self.poses_3d = torch.from_numpy(hf['pose_3d'][...])\n",
    "            self.imgs  = torch.from_numpy(hf['img'][...])\n",
    "        print(\".. done loading\")\n",
    "        self.mean, self.std = torch.FloatTensor([0.485, 0.456, 0.406]), torch.FloatTensor([0.229, 0.224, 0.225])\n",
    "        self.normalize = transforms.Normalize(self.mean, self.std)\n",
    "        self.denormalize = transforms.Compose([transforms.Normalize(mean = [ 0., 0., 0. ], std = 1/self.std),\n",
    "                                               transforms.Normalize(mean = -self.mean, std = [ 1., 1., 1. ])])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.poses_2d.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = DeviceDict(\n",
    "                  {'img': self.normalize(self.imgs[idx].float()/255),\n",
    "                  'pose_2d': self.poses_2d[idx],\n",
    "                  'pose_3d': self.poses_3d[idx]})\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoC_DwKB55VJ"
   },
   "outputs": [],
   "source": [
    "# skeleton pose definition\n",
    "# Labels are 2D (y, x) coordinate vectors, zero-based starting from the top-left pixel. They appear in the following order: \n",
    "joint_names = ['head', 'neck', 'left-shoulder', 'left-elbow', 'left-wrist', 'left-finger', 'right-shoulder', 'right-elbow', 'right-wrist', 'right-finger', 'left-hip', 'left-knee', 'left-ankle', 'left-toe', 'right-hip', 'right-knee', 'right-ankle', 'right-toe']\n",
    "# the skeleton is defined as a set of bones (pairs of skeleton joint indices):\n",
    "bones_ego_str = [('head', 'neck'), ('neck', 'left-shoulder'), ('left-shoulder', 'left-elbow'), ('left-elbow', 'left-wrist'), ('left-wrist', 'left-finger'), ('neck', 'right-shoulder'), ('right-shoulder', 'right-elbow'), ('right-elbow', 'right-wrist'), ('right-wrist', 'right-finger'), \n",
    "                 ('left-shoulder', 'left-hip'), ('left-hip', 'left-knee'), ('left-knee', 'left-ankle'), ('left-ankle', 'left-toe'), ('right-shoulder', 'right-hip'), ('right-hip', 'right-knee'), ('right-knee', 'right-ankle'), ('right-ankle', 'right-toe'), ('right-shoulder', 'left-shoulder'), ('right-hip', 'left-hip')]\n",
    "bones_ego_idx = [(joint_names.index(b[0]),joint_names.index(b[1])) for b in bones_ego_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1378,
     "status": "error",
     "timestamp": 1578901107235,
     "user": {
      "displayName": "Helge Rhodin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDx7RzVpx1g9OElwvn2vngvWxd9Sy5QCa98TE4zPg=s64",
      "userId": "13505891862888343836"
     },
     "user_tz": 480
    },
    "id": "khpc584q55VO",
    "outputId": "04b70186-bb2b-4083-8faf-1192f0fa7514"
   },
   "outputs": [],
   "source": [
    "# plotting utility functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r\"\"\"Plots skeleton pose on a matplotlib axis.\n",
    "\n",
    "        Args:\n",
    "            ax (Axis): plt axis to plot\n",
    "            pose_2d (FloatTensor): tensor of keypoints, of shape K x 2\n",
    "            bones (list): list of tuples, each tuple defining the keypoint indices to be connected by a bone \n",
    "        Returns:\n",
    "            Module: self\n",
    "\"\"\"            \n",
    "def plot_skeleton(ax, pose_2d, bones=bones_ego_idx, linewidth=2, linestyle='-'):\n",
    "    cmap = plt.get_cmap('hsv')\n",
    "    for bone in bones:\n",
    "        color = cmap(bone[1] * cmap.N // len(joint_names)) # color according to second joint index\n",
    "        ax.plot(pose_2d[bone,0], pose_2d[bone,1], linestyle, color=color, linewidth=linewidth)\n",
    "\n",
    "r\"\"\"Plots list of skeleton poses and image.\n",
    "\n",
    "        Args:\n",
    "            poses (list): list of pose tensors to be plotted\n",
    "            ax (Axis): plt axis to plot\n",
    "            bones (list): list of tuples, each tuple defining the keypoint indices to be connected by a bone \n",
    "        Returns:\n",
    "            Module: self\n",
    "\"\"\"       \n",
    "def plotPoseOnImage(poses, img, ax=plt):\n",
    "    img_pil = torchvision.transforms.ToPILImage()(img)\n",
    "    img_size = torch.FloatTensor(img_pil.size)\n",
    "    if type(poses) is not list:\n",
    "      poses = [poses]\n",
    "    linestyles = ['-', '--', '-.', ':']\n",
    "    for i, p in enumerate(poses):\n",
    "      pose_px = p*img_size\n",
    "      plot_skeleton(ax, pose_px, linestyle=linestyles[i%len(linestyles)])\n",
    "    ax.imshow(img_pil)\n",
    "\n",
    "r\"\"\"Converts a multi channel heatmap to an RGB color representation for display.\n",
    "\n",
    "        Args:\n",
    "            heatmap (tensor): of size C X H x W\n",
    "        Returns:\n",
    "            image (tensor): of size 3 X H x W\n",
    "\"\"\"       \n",
    "def heatmap2image(heatmap):\n",
    "    C,H,W = heatmap.shape\n",
    "    cmap = plt.get_cmap('hsv')\n",
    "    img = torch.zeros(3,H,W).to(heatmap.device)\n",
    "    for i in range(C):\n",
    "        color = torch.FloatTensor(cmap(i * cmap.N // C)[:3]).reshape([-1,1,1]).to(heatmap.device)\n",
    "        img = torch.max(img, color * heatmap[i]) # max in case of overlapping position of joints\n",
    "    # heatmap and probability maps might have small maximum value. Normalize per channel to make each of them visible\n",
    "    img_max, indices = torch.max(img,dim=-1,keepdim=True)\n",
    "    img_max, indices = torch.max(img_max,dim=-2,keepdim=True)\n",
    "    return img/img_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1118,
     "status": "ok",
     "timestamp": 1578892546670,
     "user": {
      "displayName": "Helge Rhodin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDx7RzVpx1g9OElwvn2vngvWxd9Sy5QCa98TE4zPg=s64",
      "userId": "13505891862888343836"
     },
     "user_tz": 480
    },
    "id": "uaOGZWro55VS",
    "outputId": "dea3041d-3076-4ccb-d20d-afe0cee10e4b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setting up the dataset and train/val splits\n",
    "path='./'\n",
    "ecds = EgoCapDataset(data_folder=path)\n",
    "\n",
    "val_ratio = 0.2\n",
    "val_size = int(len(ecds)*val_ratio)\n",
    "indices_val = list(range(0, val_size))\n",
    "indices_train = list(range(val_size, len(ecds)))\n",
    "\n",
    "val_set   = torch.utils.data.Subset(ecds, indices_val)\n",
    "train_set = torch.utils.data.Subset(ecds, indices_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1280,
     "status": "ok",
     "timestamp": 1578892547570,
     "user": {
      "displayName": "Helge Rhodin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDx7RzVpx1g9OElwvn2vngvWxd9Sy5QCa98TE4zPg=s64",
      "userId": "13505891862888343836"
     },
     "user_tz": 480
    },
    "id": "0oWc4_hlBz0A",
    "outputId": "a7bd5bfb-625a-4047-daf7-dd211b345f12"
   },
   "outputs": [],
   "source": [
    "# playing with data and plotting functions\n",
    "sample_train = train_set[100]\n",
    "sample_val = val_set[100]\n",
    "plotPoseOnImage(sample_train['pose_2d'], ecds.denormalize(sample_train['img']))\n",
    "plt.show()\n",
    "plotPoseOnImage(sample_val['pose_2d'], ecds.denormalize(sample_val['img']))\n",
    "plt.show()\n",
    "print('dataset length', len(ecds))\n",
    "print('train_set length', len(train_set))\n",
    "print('val_set length', len(val_set))\n",
    "print('pose shape',sample_train['pose_2d'].shape)\n",
    "print('img shape',sample_train['img'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATLxVpnr55VY"
   },
   "outputs": [],
   "source": [
    "# define the dataset loader (batch size, shuffling, ...)\n",
    "collate_fn_device = lambda batch : DeviceDict(torch.utils.data.dataloader.default_collate(batch)) # collate_fn_device is necessary to preserve our custom dictionary during the collection of samples fetched from the dataset into a Tensor batch. \n",
    "# Hopefully, one day, pytorch might change the default collate to pretain the mapping type. Currently all Mapping objects are converted to dict. Anyone wants to create a pull request? Would need to be changed in \n",
    "# pytorch/torch/utils/data/_utils/collate.py :     elif isinstance(data, container_abcs.Mapping): return {key: default_convert(data[key]) for key in data}\n",
    "# pytorch/torch/utils/data/_utils/pin_memory.py : if isinstance(data, container_abcs.Mapping): return {k: pin_memory(sample) for k, sample in data.items()}\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 2, num_workers=0, pin_memory=False, shuffle=True, drop_last=True, collate_fn=collate_fn_device) # Note, setting pin_memory=False to avoid the pin_memory call\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size = 2, num_workers=0, pin_memory=False, shuffle=False, drop_last=True, collate_fn=collate_fn_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVXkgRUMpZ_2"
   },
   "source": [
    "## Regression-based pose inference\n",
    "\n",
    "We provide a baseline method that regresses 2D pose straight from the image. Make sure that it runs on your hardware and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_JQWhWsn55Vd"
   },
   "outputs": [],
   "source": [
    "# define a regression network that works on dictionaries\n",
    "class RegressionNet(torch.nn.Module):\n",
    "    def __init__(self, num_joints):\n",
    "        super().__init__()\n",
    "        self.num_joints = num_joints\n",
    "        self.net = torchvision.models.resnet50(num_classes=num_joints*2)\n",
    "\n",
    "    def forward(self, dictionary):\n",
    "        return DeviceDict({'pose_2d' : self.net(dictionary['img']).reshape(-1,self.num_joints,2)})\n",
    "num_joints = len(joint_names)\n",
    "regression_network = RegressionNet(num_joints=num_joints).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1180,
     "status": "error",
     "timestamp": 1578889183615,
     "user": {
      "displayName": "Helge Rhodin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDx7RzVpx1g9OElwvn2vngvWxd9Sy5QCa98TE4zPg=s64",
      "userId": "13505891862888343836"
     },
     "user_tz": 480
    },
    "id": "aJ438TMqqx-h",
    "outputId": "033d3acc-59fe-4ac6-a1b1-77242dd8fcb2"
   },
   "outputs": [],
   "source": [
    "# training loop for regression\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "optimizer = torch.optim.Adam(regression_network.parameters(), lr=0.001)\n",
    "fig=plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "axes=fig.subplots(1,2)\n",
    "num_epochs = 10\n",
    "losses = []\n",
    "for e in range(num_epochs):\n",
    "  train_iter = iter(train_loader)\n",
    "  for i in range(len(train_loader)):\n",
    "      batch_cpu = next(train_iter)\n",
    "      batch_gpu = batch_cpu.to('cuda')\n",
    "      pred = regression_network(batch_gpu)\n",
    "      pred_cpu = pred.to('cpu')\n",
    "\n",
    "      loss = torch.nn.functional.mse_loss(pred['pose_2d'], batch_gpu['pose_2d'])\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      losses.append(loss.item())\n",
    "\n",
    "      if i%10==0:\n",
    "          # clear figures for a new update\n",
    "          for ax in axes:\n",
    "            ax.cla()\n",
    "          # plot the predicted pose and ground truth pose on the image\n",
    "          plotPoseOnImage([pred_cpu['pose_2d'][0].detach(), batch_cpu['pose_2d'][0]], \n",
    "                          ecds.denormalize(batch_cpu['img'][0]), ax=axes[0])\n",
    "          # plot the training error on a log plot\n",
    "          axes[1].plot(losses)\n",
    "          axes[1].set_yscale('log')\n",
    "          # clear output window and diplay updated figure\n",
    "          display.clear_output(wait=True)\n",
    "          display.display(plt.gcf())\n",
    "          print(\"Epoch {}, iteration {} of {} ({} %), loss={}\".format(e, i, len(train_loader), 100*i//len(train_loader), losses[-1]))\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ll9Ns_gr55Vr"
   },
   "source": [
    "## Heatmap-based pose classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5kiahQf55Vt"
   },
   "outputs": [],
   "source": [
    "# Detection network that handles dictionaries as input and output\n",
    "class HeatNetWrapper(torch.nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, dictionary):\n",
    "        return DeviceDict({'heatmap':(self.net(dictionary['img'])['out'])})\n",
    "num_joints = len(joint_names)\n",
    "det_network = HeatNetWrapper(torchvision.models.segmentation.deeplabv3_resnet50(num_classes=num_joints)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LZcaiyRHDC6C"
   },
   "outputs": [],
   "source": [
    "# Function that takes an NxKx2 pose vector (N: batch dimension, K: number of keypoints) to create stacks of heatmaps that have Gaussian distribution with the mean at the keypoint and standard deviation equal to 3.\n",
    "# The second argument specifies the output dimensions of the map. Note that the keypoints are defined in normalized coordinates, ranging from 0..1 irrespectively of the image resolution.\n",
    "import math\n",
    "\n",
    "r\"\"\"Creates a heatmap stack, with each channel having Gaussian form with mean at the pose keypoint locations\n",
    "\n",
    "        Args:\n",
    "            pose_2d (tensor): tensor of size N x K x 2, with K the number of keypoints. Keypoint locations store relative keypoint locations, i.e. both x and y coordinated in the range 0..1\n",
    "            map_size (tuple): height and width of the heatmap to be generated\n",
    "        Returns:\n",
    "            heatmap (tensor): tensor of size N x K x H x W, with K the number of keypoints\n",
    "\"\"\"       \n",
    "def pose2heatmap(pose_2d, map_size):\n",
    "  # TODO: Task I\n",
    "  pass\n",
    "\n",
    "r\"\"\"Takes a heatmap and returns the location of the maximum value in the heatmap\n",
    "\n",
    "        Args:\n",
    "            heatmap (tensor): tensor of size N x K x H x W, with K the number of keypoints\n",
    "        Returns:\n",
    "            pose (tensor): tensor of size N x K x 2, the 2D pose for each image in the batch\n",
    "\"\"\"       \n",
    "def heatmap2pose(heatmap):\n",
    "  max_alongx, _ = torch.max(heatmap, dim=-1)\n",
    "  max_alongy, _ = torch.max(heatmap, dim=-2)\n",
    "  _, max_y_index= torch.max(max_alongx, dim=-1)\n",
    "  _, max_x_index = torch.max(max_alongy, dim=-1)\n",
    "  res_y, res_x = heatmap.shape[-2:]\n",
    "  return torch.stack([max_x_index/float(res_x), max_y_index/float(res_y)],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2770,
     "status": "error",
     "timestamp": 1578892534648,
     "user": {
      "displayName": "Helge Rhodin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDx7RzVpx1g9OElwvn2vngvWxd9Sy5QCa98TE4zPg=s64",
      "userId": "13505891862888343836"
     },
     "user_tz": 480
    },
    "id": "RT0BaEh7E90z",
    "outputId": "5b2dd33a-bacb-47bb-e6ee-5b25d1eab55a"
   },
   "outputs": [],
   "source": [
    "# training loop for heatmap prediction\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "optimizer = torch.optim.Adam(det_network.parameters(), lr=0.001)\n",
    "fig=plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "axes=fig.subplots(1,4)\n",
    "losses = []\n",
    "num_epochs = 10\n",
    "for e in range(num_epochs):\n",
    "  train_iter = iter(train_loader)\n",
    "  for i in range(len(train_loader)):\n",
    "      batch_cpu = next(train_iter)\n",
    "      batch_gpu = batch_cpu.to('cuda')\n",
    "      pred_gpu = det_network(batch_gpu)\n",
    "      pred_cpu = pred_gpu.to('cpu')\n",
    "      \n",
    "      # convert between representations\n",
    "      img_shape = batch_gpu['img'].shape\n",
    "      gt_heatmap_gpu = pose2heatmap(batch_gpu['pose_2d'], img_shape[-2:])\n",
    "      pred_pose = heatmap2pose(pred_cpu['heatmap']).cpu() # note, not differentiable\n",
    "      gt_pose_max = heatmap2pose(gt_heatmap_gpu).cpu()\n",
    "\n",
    "      # optimize network\n",
    "      loss = torch.nn.functional.mse_loss(pred_gpu['heatmap'], gt_heatmap_gpu)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      losses.append(loss.item())\n",
    "        \n",
    "      # display progress\n",
    "      if i%10==0:\n",
    "          # clear figure for a new update\n",
    "          for ax in axes: \n",
    "              ax.cla()\n",
    "          # plot the ground truth and the predicted pose on top of the image\n",
    "          plotPoseOnImage([pred_pose[0], batch_cpu['pose_2d'][0]], ecds.denormalize(batch_cpu['img'][0]), ax=axes[0])\n",
    "          # plot the predicted heatmap map and the predicted pose on top\n",
    "          plotPoseOnImage([pred_pose[0]], heatmap2image(pred_cpu['heatmap'][0]), ax=axes[1])\n",
    "          # plot the reference heatmap map and the GT pose on top\n",
    "          plotPoseOnImage([gt_pose_max[0]], heatmap2image(gt_heatmap_gpu[0].cpu()), ax=axes[2])\n",
    "          # plot the current training error on a logplot\n",
    "          axes[3].plot(losses); axes[3].set_yscale('log')\n",
    "          # clear output window and diplay updated figure\n",
    "          display.clear_output(wait=True)\n",
    "          display.display(plt.gcf())\n",
    "          print(\"Epoch {}, iteration {} of {} ({} %), loss={}\".format(e, i, len(train_loader), 100*i//len(train_loader), losses[-1]))\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42K7hvKNCvxg"
   },
   "source": [
    "## Heatmap-based pose regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DxHtRr8E55V3"
   },
   "outputs": [],
   "source": [
    "def integral_heatmap_layer(dict):\n",
    "    # compute coordinate matrix\n",
    "    heatmap = dict['heatmap']\n",
    "\n",
    "    # TODO: Task II\n",
    "\n",
    "    return DeviceDict({'probabilitymap': h_norm, 'pose_2d': pose})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcBoyjlAC9Yx"
   },
   "outputs": [],
   "source": [
    "int_network = HeatNetWrapper(torchvision.models.segmentation.deeplabv3_resnet50(num_classes=num_joints)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3165,
     "status": "error",
     "timestamp": 1578893281177,
     "user": {
      "displayName": "Helge Rhodin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDx7RzVpx1g9OElwvn2vngvWxd9Sy5QCa98TE4zPg=s64",
      "userId": "13505891862888343836"
     },
     "user_tz": 480
    },
    "id": "DPbZXEE455V9",
    "outputId": "dc8e6d09-757c-48cf-92af-75899ccff180"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import time\n",
    "optimizer = torch.optim.Adam(int_network.parameters(), lr=0.001)\n",
    "fig=plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "axes=fig.subplots(1,3)\n",
    "losses = []\n",
    "num_epochs = 100\n",
    "for e in range(num_epochs):\n",
    "  train_iter = iter(train_loader)\n",
    "  for i in range(len(train_loader)):\n",
    "      batch_cpu = next(train_iter)\n",
    "      batch_gpu = batch_cpu.to('cuda')\n",
    "      pred_raw = int_network(batch_gpu)\n",
    "      pred_integral = integral_heatmap_layer(pred_raw) # note, this function must be differentiable\n",
    "\n",
    "      # optimize network\n",
    "      loss = torch.nn.functional.mse_loss(pred_integral['pose_2d'], batch_gpu['pose_2d'])\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      losses.append(loss.item())\n",
    "        \n",
    "      # plot progress\n",
    "      if i%10==0:\n",
    "          # clear figures for a new update\n",
    "          for ax in axes: \n",
    "              ax.cla()\n",
    "          pred_cpu = pred_integral.to('cpu')\n",
    "          # plot the ground truth and the predicted pose on top of the image\n",
    "          plotPoseOnImage([pred_cpu['pose_2d'][0].detach(), batch_cpu['pose_2d'][0]], ecds.denormalize(batch_cpu['img'][0]), ax=axes[0])\n",
    "          # plot the predicted probability map and the predicted pose on top\n",
    "          plotPoseOnImage([pred_cpu['pose_2d'][0].detach()], heatmap2image(pred_cpu['probabilitymap'][0]).detach(), ax=axes[1])\n",
    "          # plot the current training error on a logplot\n",
    "          axes[2].plot(losses)\n",
    "          axes[2].set_yscale('log')\n",
    "          # clear output window and diplay updated figure\n",
    "          display.clear_output(wait=True)\n",
    "          display.display(plt.gcf())\n",
    "          print(\"Epoch {}, iteration {} of {} ({} %), loss={}\".format(e, i, len(train_loader), 100*i//len(train_loader), losses[-1]))\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkeghnvS55WC"
   },
   "outputs": [],
   "source": [
    "# TODO: Task III, validation, which approach is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment2_V0_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
